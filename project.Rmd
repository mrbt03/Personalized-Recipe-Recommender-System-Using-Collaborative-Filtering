---
title: "R Notebook"
output: html_notebook
---

```{r}
rm(list=ls())
setwd("C:/Users/marko/Contacts/Desktop/IEMS 308")
# Load necessary libraries
#install.packages("recommenderlab")
library(recommenderlab)
library(tidytext)
library(dplyr)
```

Prediction:

Based on the dataset, considering there are much less items (200) than users (10,000), Item based collaborative filtering will be faster than user based collaborative filtering.

```{r}
# Read the data
m = read.csv("clicksamp.csv") %>% mutate(logcount = log(count))
r = as(cast_sparse(m, id, recipeid, logcount), "realRatingMatrix")
set.seed(12345)
(e = evaluationScheme(r, method="split", train=0.7, given=-3))
```

#### UBCF: Z-score, cosine

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Cosine Similarity with Z-score Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")
```

#### UBCF: Z-score, pearson

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "pearson"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
   n
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Pearson Similarity with Z-score Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")
```

#### UBCF: Z-score, jaccard

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "jaccard"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Jaccard Similarity with Z-score Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")
```

#### UBCF: center, cosine

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "center"
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Cosine Similarity with Centering", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

```

#### UBCF: center, pearson

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "center"
similarity_method <- "pearson"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Pearson Similarity with Center Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

```

#### UBCF: center, jaccard

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "center"
similarity_method <- "jaccard"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Jaccard Similarity with Center Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

```

```{r}
# We're focusing on cosine similarity with centering
normalize_method <- "center"
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
# Since we are now considering nn values from 80 to 100, we need 21 slots (80, 81, ..., 100)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # Adjust nn to vary from 1 to 100

# Loop over each nn value
for (i in 1:length(nn_values)) {
  nn = nn_values[i]
  
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[i] <- mae_value  # Use 'i' to correctly index the position in mae_values
}

# Prepare the data for plotting, now correctly mapping nn values to their MAE
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Cosine Similarity with Centering", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

```

#### UBCF: Z-score, cosine

```{r}
# We're focusing on cosine similarity with centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Cosine Similarity with Z-score Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### UBCF: Null, cosine

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5
rmse_values <- numeric(length=100)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn] <- mae_value
  rmse_values[nn] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Cosine Similarity with Null Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")

```

#### UBCF: Null, pearson

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL
similarity_method <- "pearson"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5
rmse_values <- numeric(length=100)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn] <- mae_value
  rmse_values[nn] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Pearson Similarity with Null Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### UBCF: Null, jaccard

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL
similarity_method <- "jaccard"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 5
rmse_values <- numeric(length=100)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "UBCF", parameter = list(nn = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn] <- mae_value
  rmse_values[nn] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting nn vs MAE
plot(results_df$nn, results_df$MAE, type='b', main="Jaccard Similarity with Null Normalization", xlab="Number of Nearest Neighbors (NN)", ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the number of nearest neighbors and the minimum MAE
cat("Best Number of Nearest Neighbors (NN):", results_df$nn[min_mae_row], "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### Which normalization works best? What number of nearest neighbors (nn) works best?

No normalization and cosine similarity works best.
For IBCF: 

#### IBCF: Null, cosine
```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 150)
nn_values <- 1:150  # nn will vary from 1 to 100
rmse_values <- numeric(length=150)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn] <- mae_value
  rmse_values[nn] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Cosine Similarity and Null Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")
# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k)::", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")

```

#### IBCF: Null, pearson

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL
similarity_method <- "pearson"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100
rmse_values <- numeric(length=100)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn] <- mae_value
  rmse_values[nn] <- rmse_value
}


# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Pearson Similarity and NUll Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")

```

#### IBCF: Null, jaccard

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL
similarity_method <- "jaccard"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100
rmse_values <- numeric(length=100)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn] <- mae_value
  rmse_values[nn] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Jaccard Similarity and NULL Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")
# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")

```

#### IBCF: center, cosine

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "center"
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100
rmse_values <- numeric(length=100)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Cosine Similarity and Center Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### IBCF: center, pearson

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "center"
similarity_method <- "pearson"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Pearson Similarity and Center Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### IBCF: center, jaccard

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "center"
similarity_method <- "jaccard"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Jaccard Similarity and center Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

```

#### IBCF: Z-score, cosine

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "cosine"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Cosine Similarity and Z-score Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### IBCF: Z-score, pearson

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "pearson"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Pearson Similarity and Z-score Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### IBCF: Z-score, jaccard

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "Z-score"
similarity_method <- "jaccard"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 100)
nn_values <- 1:100  # nn will vary from 1 to 100

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  algorithm <- list(name = "IBCF", parameter = list(k = nn, method = similarity_method, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  # Store MAE in the vector
  mae_values[nn] <- mae_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values)

# Plotting k vs MAE for IBCF
plot(results_df$nn, results_df$MAE, type='b', 
     main="IBCF with Jaccard Similarity and Z-score Normalization", 
     xlab="Number of Nearest Neighbors (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of nearest neighbors (k) and the minimum MAE
cat("Optimal Number of Nearest Neighbors (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")


```

#### Again, what normalization works best? How many nns?

For SVD

#### SVD: NULL

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- NULL

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 98)
nn_values <- 2:99  # nn will vary from 2 to 100
rmse_values <- numeric(length=98)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  #browser()
  algorithm <- list(name = "SVD", parameter = list(k = nn, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn-1] <- mae_value
  rmse_values[nn-1] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting k vs MAE for SVD
plot(results_df$nn, results_df$MAE, type='b', 
     main="SVD with Different k Values (NULL)", 
     xlab="Number of Singular Values (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")
```

#### SVD: center

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "center"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 98)
nn_values <- 2:99  # nn will vary from 2 to 100
rmse_values <- numeric(length=98)

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  #browser()
  algorithm <- list(name = "SVD", parameter = list(k = nn, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn-1] <- mae_value
  rmse_values[nn-1] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Plotting k vs MAE for SVD
plot(results_df$nn, results_df$MAE, type='b', 
     main="SVD with Different k Values (center)", 
     xlab="Number of Singular Values (k)", 
     ylab="Mean Absolute Error (MAE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")
```

#### SVD: Z-score

```{r}
# We're focusing on cosine similarity with no centering, so let's set these directly
normalize_method <- "Z-score"

# Initialize a vector to store MAE values for varying nn (number of nearest neighbors)
mae_values <- numeric(length = 98)
rmse_values <- numeric(length=98)
nn_values <- 2:99  # nn will vary from 2 to 100

# Loop over each nn value
for (nn in nn_values) {
  # Define the recommender model using cosine similarity with centering
  #browser()
  algorithm <- list(name = "SVD", parameter = list(k = nn, normalize = normalize_method))
  
  # Evaluate the recommender model
  eval_results <- evaluate(e, list(algorithm), type = "ratings")
  
  # Assuming you want to extract MAE for each configuration
  mae_value <- eval_results[[1]]@results[[1]]@cm[3]
  rmse_value <- eval_results[[1]]@results[[1]]@cm[1]
  # Store MAE in the vector
  mae_values[nn-1] <- mae_value
  rmse_values[nn-1] <- rmse_value
}

# Prepare the data for plotting
results_df <- data.frame(nn = nn_values, MAE = mae_values, RMSE= rmse_values)

# Set up plotting area to have 2 plots side by side
par(mfrow=c(1, 2))

# Plotting k vs MAE for SVD
plot(results_df$nn, results_df$MAE, type='b', 
     main="SVD with Different k Values (Z-score)", 
     xlab="Number of Singular Values (k)", 
     ylab="Mean Absolute Error (MAE)")

# Plotting k vs RMSE for SVD
plot(results_df$nn, results_df$RMSE, type='b', 
     main="SVD with Different k Values (Z-score)", 
     xlab="Number of Singular Values (k)", 
     ylab="Root Mean Square Error (RMSE)")

# Find the row with the minimum MAE
min_mae_row <- which.min(results_df$MAE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_mae_row], 
    "- Minimum MAE:", results_df$MAE[min_mae_row], "\n")

# Find the row with the minimum MAE
min_RMSE_row <- which.min(results_df$RMSE)

# Print the optimal number of singular values (k) and the minimum MAE
cat("Optimal Number of Singular Values (k):", results_df$nn[min_RMSE_row], 
    "- Minimum RMSE:", results_df$RMSE[min_RMSE_row], "\n")


```

#### Again, which normalization and how many nns? Take your best parameters from each of the models and run them together with random and popular. Which method do you suggest?

Optimal Number of Nearest Neighbors (k): 95 - Minimum MAE: 0.229515 Optimal Number of Nearest Neighbors (k): 149 - Minimum RMSE: 0.3438263

Optimal Number of Singular Values (k): 22 - Minimum MAE: 0.2410502 Optimal Number of Singular Values (k): 11 - Minimum RMSE: 0.3262573

Best Number of Nearest Neighbors (NN): 54 - Minimum MAE: 0.2382558 Best Number of Nearest Neighbors (NN): 100 - Minimum RMSE: 0.3558985

```{r}
set.seed(12345)
algorithms = list(
"svd22" = list(name="SVD", param=list(k=22,normalize=NULL)),
"svd11" = list(name="SVD", param=list(k=11,normalize=NULL)),
"IBCF95Cosine" = list(name="IBCF", param=list(k=95,normalize=NULL,method='cosine')),
"IBCF19# Set up plotting area to have 2 plots side by side
par(mfrow=c(1, 2))0Pearson" = list(name="IBCF", param=list(k=190,normalize=NULL,method='pearson')),
"UBCF54" = list(name="UBCF", param=list(nn=54,normalize=NULL,method='cosine')),
#"UBCF100" = list(name="UBCF", param=list(nn=100,normalize=NULL,method='cosine')),
"Popular" = list(name="POPULAR")
)
(results = evaluate(e, algorithms, type="ratings"))
plot(results, col=1:6, annotate=T, legend="bottomleft")
```
```{r}
# Extract the training data from the evaluation scheme
train_data <- getData(e, "train")

# Train the IBCF model using the training data with k = 95
k <- 95
normalize_method <- NULL
similarity_method <- "cosine"

ibcf_model_95 <- Recommender(train_data, method = "IBCF", parameter = list(k = k, method = similarity_method, normalize = normalize_method))

# Save the trained model to a file
saveRDS(ibcf_model_95, file = "ibcf_recommender_model_k95.rds")
cat("IBCF model with k = 95 saved to 'ibcf_recommender_model_k95.rds'\n")
```

